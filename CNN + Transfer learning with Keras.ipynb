{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzORuUclqa0K"
   },
   "source": [
    "# Convolutional neural network with Keras\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwzpkLvUqEoX"
   },
   "source": [
    "## **Data import**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bm66w3z-umCD"
   },
   "source": [
    "Data can be downloaded from the following link:\n",
    "[fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n",
    "Data is to be uploaded to Gdrive and kept in folder data/fashion/ for the code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ia1BDKLVK3Of"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "!pip install -U -q PyDrive\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOOq9iOrK9Qc"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from google.colab import auth\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "# Mount data from Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTk3ok4SLcEc"
   },
   "outputs": [],
   "source": [
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "google_auth = GoogleAuth()\n",
    "google_auth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(google_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXdw9NkZLn6v"
   },
   "outputs": [],
   "source": [
    "# Mnist reader function sourced from https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WfwdffwiLq7W"
   },
   "outputs": [],
   "source": [
    "# Set data to X-train/X-test and y_train/y_test\n",
    "X_train, y_train = load_mnist('gdrive/My Drive/data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('gdrive/My Drive/data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoTPr9xZLtRg"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of X_train is :\",X_train.shape)\n",
    "print(\"Shape of Y_train is :\",y_train.shape)\n",
    "print(\"Shape of X_test is :\",X_test.shape)\n",
    "print(\"Shape of Y_test is :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPTpQbKjLv8K"
   },
   "outputs": [],
   "source": [
    "# Class names in Mnist-fashion dataset\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Find the number of images in each class\n",
    "for i in range(0,10):\n",
    "  print(\"Number of %s images: %i\" % (class_names[i], len(y_train[y_train == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKeF0F84L0Ey"
   },
   "outputs": [],
   "source": [
    "# Display the gray scale images in a 10*10 grid\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "923lGtkWL3Dy"
   },
   "outputs": [],
   "source": [
    "# Reshape 2d to 3d array\n",
    "im_rows = 28 # height\n",
    "im_cols = 28 # width\n",
    "batch_size = 512 # size of batch\n",
    "im_shape = (im_rows, im_cols, 1) # shape of array 28*28*1\n",
    "\n",
    "# reshape array\n",
    "X_train = X_train.reshape(X_train.shape[0], *im_shape) \n",
    "X_test = X_test.reshape(X_test.shape[0], *im_shape)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train.shape))\n",
    "print('X_test shape:{}'.format(X_test.shape))\n",
    "\n",
    "num_classes = 10\n",
    "# convert number to categories\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6zgN2kZqO7y"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fycOQbFL6vd"
   },
   "outputs": [],
   "source": [
    "# To train on ResNet50 model pad zeros to make 28*28*1 array to 32*32*3 array\n",
    "X_train = np.pad(X_train,((0,0),(2,2),(2,2),(1,1)), mode='constant', constant_values=0)\n",
    "X_test = np.pad(X_test,((0,0),(2,2),(2,2),(1,1)), mode='constant', constant_values=0)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train.shape))\n",
    "print('X_test shape:{}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW9UqlolrzPX"
   },
   "source": [
    "### Add two dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8BF28qVL94N"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the ResNet50 model with pre-trained weights. \n",
    "# Do not include fully connected layers as num of classes in \n",
    "# Mnist fashion dataset is 10 and the model was trained for 50 classes\n",
    "ResNet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    \n",
    "# Add a global spatial average pooling layer\n",
    "x = ResNet50_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Fully-connected layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# Fully connected output/classification layer\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create a full network with the new layers\n",
    "ResNet50_transfer = Model(input=ResNet50_base.input, output=predictions)\n",
    "\n",
    "# Freeze all layers of the ResNet50 model\n",
    "for layer in ResNet50_transfer.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Compile the model\n",
    "ResNet50_transfer.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cAQ9iIVsjpF"
   },
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ap-UvGmdsr1y"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = ResNet50_transfer.fit(X_train, y_train,\n",
    "         batch_size=256, epochs=10,\n",
    "         validation_split=0.2,\n",
    "         verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2Y5R88etCuF"
   },
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhU7n1qnOCBv"
   },
   "outputs": [],
   "source": [
    "# Training and validation accuracy\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Training and validation loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,8])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cZXDo3StOcT"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIYE1cjXN6mS"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "score = ResNet50_transfer.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (ResNet50_transfer.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltOcVgcyOTIQ"
   },
   "source": [
    "###  Unfreeze last two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pmuLd1eOP8d"
   },
   "outputs": [],
   "source": [
    "# Set last two layers for training\n",
    "for layer in ResNet50_transfer.layers[176:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Compile the model\n",
    "ResNet50_transfer.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-1E6MtJtcpm"
   },
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWTml0T7taiD"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = ResNet50_transfer.fit(X_train, y_train,\n",
    "         batch_size=256, epochs=10,\n",
    "         validation_split=0.2,\n",
    "         verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6W53_cFObhn"
   },
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdzGiBvGOzf4"
   },
   "outputs": [],
   "source": [
    "# Training and validation accuracy\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Training and validation loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,8])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYpP6UhRO8iF"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHYuFGjbO8ED"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "score = ResNet50_transfer.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (ResNet50_transfer.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwjpFBg5PEMk"
   },
   "source": [
    "## CNN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4T7qyjGS14S"
   },
   "outputs": [],
   "source": [
    "# Set data to X-train/X-test and y_train/y_test\n",
    "X_train, y_train = load_mnist('gdrive/My Drive/data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('gdrive/My Drive/data/fashion', kind='t10k')\n",
    "\n",
    "# Reshape 2d to 3d array\n",
    "im_rows = 28 # height\n",
    "im_cols = 28 # width\n",
    "batch_size = 512 # size of batch\n",
    "im_shape = (im_rows, im_cols, 1) # shape of array 28*28*1\n",
    "\n",
    "# reshape array\n",
    "X_train = X_train.reshape(X_train.shape[0], *im_shape) \n",
    "X_test = X_test.reshape(X_test.shape[0], *im_shape)\n",
    "\n",
    "num_classes = 10\n",
    "# convert number to categories\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train.shape))\n",
    "print('X_test shape:{}'.format(X_test.shape))\n",
    "\n",
    "print('Y_train shape:{}'.format(y_train.shape))\n",
    "print('y_test shape:{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5P43k4iouJ1O"
   },
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGBVoQ4jSbBO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "name = 'cnn_base_model'\n",
    "cnn_base_model = Sequential([ # Stack layers in a sequence\n",
    "    Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=im_shape, name='Conv2D-1'),#filter windows-32, kernel_window_size-2*2, padding - same for zero padding\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool-1'), # Max_pool_window_size - 2\n",
    "    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='Conv2D-2'),#filter_windows-64, kernel_window_size-3*3, padding - same for zero padding\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool-2'), # Max_pool_window_size - 2\n",
    "    Flatten(name='flatten'), # Flatten array to single dimension\n",
    "    Dense(128, activation='relu', name='Dense-1'), # Fully connected layer with relu activation\n",
    "    Dropout(0.3, name='Dropout-1'),\n",
    "    Dense(10, activation='softmax', name='Output') # Fully connected layer with softmax activation\n",
    "], name = name)\n",
    "  \n",
    "cnn_base_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer= 'adam',\n",
    "    metrics=['accuracy']\n",
    "  ) \n",
    "\n",
    "cnn_base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2x8KDe-zx90s"
   },
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7gIkP62SaZO"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "cnn_base_model.fit(\n",
    "          X_train, y_train, batch_size=batch_size,\n",
    "          epochs=10, verbose=1,\n",
    "          validation_data=(X_test, y_test)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Nu9pWFmyIin"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgBqRRtUxusl"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "score = cnn_base_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"5 layer CNN %s: %.2f%%\" % (cnn_base_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fINbKBcYy62G"
   },
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvKTW1EB5l50"
   },
   "outputs": [],
   "source": [
    "# Set data to X-train/X-test and y_train/y_test\n",
    "X_train, y_train = load_mnist('gdrive/My Drive/data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('gdrive/My Drive/data/fashion', kind='t10k')\n",
    "\n",
    "# Reshape 2d to 3d array\n",
    "im_rows = 28 # height\n",
    "im_cols = 28 # width\n",
    "im_shape = (im_rows, im_cols, 1) # shape of array 28*28*1\n",
    "\n",
    "# reshape array\n",
    "X_train = X_train.reshape(X_train.shape[0], *im_shape) \n",
    "X_test = X_test.reshape(X_test.shape[0], *im_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51thv3KRyiaC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [500, 750, 1000]\n",
    "epochs = [40, 60]\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "dropout_rate = [0.2, 0.3, 0.4]\n",
    "param_grid = dict(batch_size=batch_size, epochs = epochs, learn_rate = learn_rate, dropout_rate = dropout_rate)\n",
    "\n",
    "def create_model(learn_rate = 0.1, dropout_rate = 0.01):\n",
    "  name = 'cnn_model'\n",
    "  cnn_model = Sequential([# Stack sequential layers\n",
    "    Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=im_shape, name='Conv2D-1'),#filter windows-32, kernel_window_size-2*2, padding - same for zero padding\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool-1'), # Max_pool_window_size - 2\n",
    "    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', name='Conv2D-2'),#filter_windows-64, kernel_window_size-3*3, padding - same for zero padding\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool-2'), # Max_pool_window_size - 2\n",
    "    Flatten(name='flatten'), # Flatten array to single dimension\n",
    "    Dense(128, activation='relu', name='Dense-1'), # Fully connected layer with relu activation\n",
    "    Dropout(dropout_rate, name='Dropout-1'),\n",
    "    Dense(10, activation='softmax', name='Output') # Fully connected layer with softmax activation\n",
    "  ], name = name)\n",
    "  cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', # Cost function\n",
    "    optimizer= Adam(lr=learn_rate), # Optimization algorithm\n",
    "    metrics=['accuracy'] # Evaluation metric\n",
    "  ) \n",
    "  return cnn_model\n",
    "\n",
    "# create model\n",
    "cnn_model = KerasClassifier(build_fn=create_model, nb_epoch=10, verbose=0)\n",
    "\n",
    "grid = GridSearchCV(estimator=cnn_model, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=1) # Set the model and hyperparamter grid\n",
    "grid_result = grid.fit(X_train, y_train) # Train the model\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score'] # Mean test set accuracy\n",
    "stds = grid_result.cv_results_['std_test_score'] \n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) # The best hyperparamter combination is 'batch_size': 1000, 'dropout_rate': 0.4, 'epochs': 40, 'learn_rate': 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJMiSIHRSdMi"
   },
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqW96Cm8O5WM"
   },
   "outputs": [],
   "source": [
    "# Set data to X-train/X-test and y_train/y_test\n",
    "X_train, y_train = load_mnist('gdrive/My Drive/data/fashion', kind='train')\n",
    "X_test, y_test = load_mnist('gdrive/My Drive/data/fashion', kind='t10k')\n",
    "\n",
    "# Reshape 2d to 3d array\n",
    "im_rows = 28 # height\n",
    "im_cols = 28 # width\n",
    "batch_size = 1024 # size of training batches\n",
    "epochs = 40 # number of times data passes through forward and backward propogation\n",
    "im_shape = (im_rows, im_cols, 1) # shape of array 28*28*1\n",
    "\n",
    "# reshape array\n",
    "X_train = X_train.reshape(X_train.shape[0], *im_shape) \n",
    "X_test = X_test.reshape(X_test.shape[0], *im_shape)\n",
    "\n",
    "num_classes = 10\n",
    "# convert number to categories\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train.shape))\n",
    "print('X_test shape:{}'.format(X_test.shape))\n",
    "\n",
    "print('Y_train shape:{}'.format(y_train.shape))\n",
    "print('y_test shape:{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5q84dzYoyJu"
   },
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hyuDIDNlPJIP"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.optimizers import Adam, Nadam, Adadelta, SGD\n",
    "\n",
    "def create_model(optimizer = 'adam'):\n",
    "  name = 'cnn_model_14L' # Set name for the model\n",
    "  cnn_model_14L = Sequential([\n",
    "      Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=im_shape, name='Conv2D-1'), #filter windows-32, kernel_window_size-3*3\n",
    "      Conv2D(filters=64, kernel_size=3, activation='relu', name='Conv2D-2'),#filter windows-64, kernel_window_size-3*3\n",
    "      Conv2D(filters=128, kernel_size=3, activation='relu', name='Conv2D-3'),#filter windows-128, kernel_window_size-3*3\n",
    "      MaxPooling2D(pool_size=2, name='MaxPool'),# Max_pool_window_size - 2\n",
    "      Conv2D(filters=128, kernel_size=3, activation='relu', name='Conv2D-4'),#filter windows-128, kernel_window_size-3*3    \n",
    "      Dropout(0.25, name='Dropout-1'),# drop 0.25 of the units\n",
    "      Conv2D(filters=256, kernel_size=3, activation='relu', name='Conv2D-5'),#filter windows-256, kernel_window_size-3*3  \n",
    "      Dropout(0.4, name='Dropout-2'), #drop 0.4 of the units\n",
    "      Flatten(name='flatten'), # Flatten to 1D array\n",
    "      Dense(256, activation='relu', name='Dense-1'), # Linear fully connected unit.\n",
    "      Dense(256, activation='relu', name='Dense-2'), # Linear fully connected unit.\n",
    "      Dropout(0.4, name='Dropout-3'), # Dropout 0.4 of the units\n",
    "      Dropout(0.4, name='Dropout-4'), # Dropout 0.5 of the units\n",
    "      Dense(10, activation='softmax', name='Output') # Softmax activation for multiclass classification\n",
    "    ], name = name)\n",
    "  \n",
    "  cnn_model_14L.compile( # Compile the model\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer= optimizer,\n",
    "    metrics=['accuracy']\n",
    "  ) \n",
    "  return cnn_model_14L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrJnpYZNoqbd"
   },
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPIghr2aPdiu"
   },
   "outputs": [],
   "source": [
    "optimizers = ['adam', 'sgd', 'adadelta', 'nadam'] # Define various optimizers\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    if optimizer == 'adam':\n",
    "      # Create model\n",
    "      cnn_model_14L_adam = create_model(optimizer = optimizer)\n",
    "      # Train model\n",
    "      history_adam = cnn_model_14L_adam.fit(\n",
    "          X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test, y_test)\n",
    "      )\n",
    "      # Model evaluation\n",
    "      score = cnn_model_14L_adam.evaluate(X_test, y_test, verbose=0)\n",
    "      print(\"Adam optimizer %s: %.2f%%\" % (cnn_model_14L_adam.metrics_names[1], score[1]*100))\n",
    "\n",
    "    if optimizer == 'sgd':\n",
    "      # Create model\n",
    "      cnn_model_14L_sgd = create_model(optimizer = optimizer) \n",
    "      # Train model\n",
    "      history_sgd = cnn_model_14L_sgd.fit(\n",
    "          X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test, y_test)\n",
    "      )\n",
    "      # Model evaluation\n",
    "      score = cnn_model_14L_sgd.evaluate(X_test, y_test, verbose=0)\n",
    "      print(\"Sgd optimizer %s: %.2f%%\" % (cnn_model_14L_sgd.metrics_names[1], score[1]*100))\n",
    "\n",
    "    if optimizer == 'adadelta':\n",
    "      # Create model\n",
    "      cnn_model_14L_adadelta = create_model(optimizer = optimizer)\n",
    "      # Train model\n",
    "      history_adadelta = cnn_model_14L_adadelta.fit(\n",
    "          X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test, y_test)\n",
    "      )\n",
    "      # Model evaluation\n",
    "      score = cnn_model_14L_adadelta.evaluate(X_test, y_test, verbose=0)\n",
    "      print(\"AdaDelta optimizer %s: %.2f%%\" % (cnn_model_14L_adadelta.metrics_names[1], score[1]*100))\n",
    "\n",
    "    if optimizer == 'nadam':\n",
    "      # Create model\n",
    "      cnn_model_14L_nadam = create_model(optimizer = optimizer)\n",
    "      # Train model\n",
    "      history_nadam = cnn_model_14L_nadam.fit(\n",
    "          X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test, y_test)\n",
    "     )\n",
    "      # Model evaluation\n",
    "      score = cnn_model_14L_nadam.evaluate(X_test, y_test, verbose=0)\n",
    "      print(\"Nadam optimizer %s: %.2f%%\" % (cnn_model_14L_nadam.metrics_names[1], score[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEsr_HJ_SMp1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adam_acc = history_adam.history['val_acc']\n",
    "sgd_acc = history_sgd.history['val_acc']\n",
    "adadelta_acc = history_adadelta.history['val_acc']\n",
    "nadam_acc = history_nadam.history['val_acc']\n",
    "\n",
    "adam_val_loss = history_adam.history['val_loss']\n",
    "sgd_val_loss = history_sgd.history['val_loss']\n",
    "adadelta_val_loss = history_adadelta.history['val_loss']\n",
    "nadam_val_loss = history_nadam.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(adam_acc, label='Adam')\n",
    "plt.plot(sgd_acc, label='Sgd')\n",
    "plt.plot(adadelta_acc, label='AdaDelta')\n",
    "plt.plot(nadam_acc, label='Nadam')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Validation accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.plot(adam_val_loss, label='Adam')\n",
    "plt.plot(sgd_val_loss, label='Sgd')\n",
    "plt.plot(adadelta_val_loss, label='AdaDelta')\n",
    "plt.plot(nadam_val_loss, label='Nadam')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.ylim([0,1])\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WG4Yi8UirLdK"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "cnn_model_14L_adadelta = create_model(optimizer = 'adadelta')\n",
    "# Train model\n",
    "history_adadelta = cnn_model_14L_adadelta.fit(\n",
    "    X_train, y_train, batch_size=batch_size,\n",
    "    epochs=epochs, verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "# Model evaluation\n",
    "score = cnn_model_14L_adadelta.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"AdaDelta optimizer %s: %.2f%%\" % (cnn_model_14L_adadelta.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIR5LPUhwqcq"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    " score = cnn_model_14L_adadelta.evaluate(X_test, y_test, verbose=0)\n",
    "      print(\"AdaDelta optimizer %s: %.2f%%\" % (cnn_model_14L_adadelta.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1-MYr-elVsb"
   },
   "source": [
    "#### Save and load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YlzAXw02zy9"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Save model to disk\n",
    "# # serialize model to JSON\n",
    "# model_json = cnn_model_14L_adadelta.to_json()\n",
    "# with open(\"cnn_model_14L_adadelta.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# cnn_model_14L_adadelta.save_weights(\"cnn_model_14L_adadelta.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWtQDtqK3KWd"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('cnn_model_14L_adadelta.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"cnn_model_14L_adadelta.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04n_l1q0-lcr"
   },
   "source": [
    "#### Misclassified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uj_pL-XS6S3L"
   },
   "outputs": [],
   "source": [
    "predicted_class_labels = cnn_model_14L_adadelta.predict_classes(X_test)\n",
    "misclassified_labels = y_test[np.isclose(predicted_class_labels, y_test) != True]\n",
    "\n",
    "# Class names in Mnist-fashion dataset\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "for i in range(1,10):\n",
    "  print(\"{}:{}\".format(class_names[i],sum(misclassified_labels == i)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearningAT1B.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
